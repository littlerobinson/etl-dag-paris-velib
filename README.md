# Paris CityMapper ETL Pipeline ğŸ—¼

## Overview ğŸŒŸ

This project implements an automated ETL (Extract, Transform, Load) pipeline using Apache Airflow to collect and process two main data sources:

- Weather data from OpenWeatherMap API ğŸŒ¤ï¸
- Velib (bike-sharing) station status from Velib Metropole API ğŸš²

The pipeline runs hourly to provide up-to-date information about weather conditions and bike availability in Paris.

## Architecture ğŸ—ï¸

The project uses the following technologies:

- Apache Airflow 2.10.5 for workflow orchestration ğŸ”„
- PostgreSQL for data storage ğŸ’¾
- Amazon S3 for intermediate data storage â˜ï¸
- Redis for Airflow's Celery Executor ğŸ”§
- Docker and Docker Compose for containerization ğŸ³

## Project Structure ğŸ“

```
citymapper_workflow_etl/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ etl_dag.py       # Main DAG file
â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â””â”€â”€ s3_to_postgres.py # Custom operator
â”‚   â”œâ”€â”€ docker-compose.yaml   # Docker configuration
â”‚   â””â”€â”€ .env                  # Environment variables
```

## Data Pipeline ğŸ”„

The DAG consists of two parallel branches:

### Weather Branch ğŸŒ¤ï¸

1. Fetch weather data from OpenWeatherMap API
2. Transform data into structured CSV format
3. Create weather table in PostgreSQL (if not exists)
4. Load data into PostgreSQL

### Station Status Branch ğŸš²

1. Fetch Velib station status from API
2. Transform data into structured CSV format
3. Create station status table in PostgreSQL (if not exists)
4. Load data into PostgreSQL

## Prerequisites ğŸ“‹

- Docker and Docker Compose installed
- AWS credentials for S3 access
- OpenWeatherMap API key
- PostgreSQL connection details

![alt text](image-1.png)

## Airflow Configuration ğŸ”

### Variables

Set the following variables in the Airflow Admin Interface (Admin > Variables):

```
AWS_DEFAULT_REGION    # Your AWS region (e.g., eu-west-1)
OPENWEATHERMAP_API    # Your OpenWeatherMap API key
S3BucketName          # Your S3 bucket name
```

### Connections

Configure the following connections in Airflow (Admin > Connections):

1. AWS Connection (`aws_citymapper`):

   - Conn Type: Amazon Web Services
   - Configure with your AWS credentials

2. PostgreSQL Connection (`postgres_citymapper`):
   - Conn Type: Postgres
   - Configure with your database credentials

## Installation & Setup ğŸš€

1. Clone the repository:

```bash
git clone <repository-url>
cd citymapper_workflow_etl
```

2. Start the Airflow services:

```bash
cd airflow
docker compose up airflow-init
docker-compose up --build
```

4. Access Airflow web interface:

```
http://localhost:8080
Username: airflow
Password: airflow
```

## DAG Structure ğŸ“Š

```
start
  â”œâ”€â”€ weather_branch
  â”‚   â”œâ”€â”€ fetch_weather_data
  â”‚   â”œâ”€â”€ transform_weather_data
  â”‚   â”œâ”€â”€ create_weather_table
  â”‚   â””â”€â”€ transfer_weather_data
  â”‚
  â”œâ”€â”€ station_status_branch
  â”‚   â”œâ”€â”€ fetch_station_status_data
  â”‚   â”œâ”€â”€ transform_station_status_data
  â”‚   â”œâ”€â”€ create_station_status_table
  â”‚   â””â”€â”€ transfer_station_status_data
  â”‚
  â””â”€â”€ end
```

## Monitoring ğŸ“ˆ

- Access the Airflow web interface to monitor DAG runs
- Check task logs for detailed execution information
- Monitor PostgreSQL database for data consistency
- Check S3 bucket for intermediate data files

## Contributing ğŸ¤

Feel free to submit issues and enhancement requests!

## License ğŸ“œ

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.
